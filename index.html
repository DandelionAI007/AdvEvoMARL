<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning – Pan et al.">
  <meta name="description" content="AdvEvo-MARL introduces a co-evolutionary multi-agent reinforcement learning framework to internalize safety in LLM-based multi-agent systems, achieving attack success rates below 20% while preserving or improving task utility.">
  <meta name="keywords" content="multi-agent reinforcement learning, adversarial co-evolution, safety, large language models, internalized safety">
  <meta name="author" content="Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="AdvEvo-MARL Project Page">
  <meta property="og:title" content="AdvEvo-MARL: Shaping Internalized Safety">
  <meta property="og:description" content="Co-evolutionary MARL for embedding safety in LLM-based multi-agent systems: ASR < 20% with maintained/improved task utility.">
  <meta property="og:url" content="https://YOUR_DOMAIN.com/advevo-marl">
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="AdvEvo-MARL – Research Preview">
  <meta property="article:published_time" content="2025-10-02T00:00:00.000Z">
  <meta property="article:author" content="Zhenyu Pan">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="multi-agent reinforcement learning">
  <meta property="article:tag" content="adversarial co-evolution">
  
  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@FIRST_AUTHOR_TWITTER">
  <meta name="twitter:title" content="AdvEvo-MARL: Shaping Internalized Safety">
  <meta name="twitter:description" content="Co-evolutionary MARL to internalize safety: ASR < 20% and task utility maintained or improved.">
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="AdvEvo-MARL – Research Preview">
  
  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning">
  <meta name="citation_author" content="Pan, Zhenyu">
  <meta name="citation_author" content="Zhang, Yiting">
  <meta name="citation_author" content="Liu, Zhuo">
  <meta name="citation_author" content="Tang, Yunlong">
  <meta name="citation_author" content="Zhang, Zeliang">
  <meta name="citation_author" content="Luo, Haozheng">
  <meta name="citation_author" content="Han, Yuwei">
  <meta name="citation_author" content="Zhang, Jianshu">
  <meta name="citation_author" content="Wu, Dennis">
  <meta name="citation_author" content="Chen, Hong-Yu">
  <meta name="citation_author" content="Lu, Haoran");
  <meta name="citation_author" content="Fang, Haoyang");
  <meta name="citation_author" content="Li, Manling");
  <meta name="citation_author" content="Xu, Chenliang");
  <meta name="citation_author" content="Yu, Philip S.");
  <meta name="citation_author" content="Liu, Han");
  <meta name="citation_publication_date" content="2025");
  <meta name="citation_journal_title" content="arXiv preprint");
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2510.01586.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  
  <title>AdvEvo-MARL | Pan et al.</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
    "description": "AdvEvo-MARL introduces a co-evolutionary multi-agent RL framework to internalize safety in LLM-based multi-agent systems, achieving attack success rates below 20% while preserving or improving task utility.",
    "author": [
      {
        "@type": "Person",
        "name": "Zhenyu Pan",
        "affiliation": {
          "@type": "Organization",
          "name": "Northwestern University"
        }
      },
      {
        "@type": "Person",
        "name": "Yiting Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Illinois at Chicago"
        }
      }
    ],
    "datePublished": "2025-10-02",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://YOUR_DOMAIN.com/advevo-marl",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["multi-agent reinforcement learning","adversarial co-evolution","safety","large language model agents"],
    "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions; (ii) external guard modules. The former often under-performs because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Attackers and defenders evolve together; a public baseline reduces variance and fosters cooperation. Across attack scenarios, AdvEvo-MARL keeps attack success rate (ASR) below 20% while maintaining or improving task accuracy (up to +3.67%).",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/advevo-marl"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Multi-Agent Reinforcement Learning"
      },
      {
        "@type": "Thing",
        "name": "AI Safety"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "AdvEvo-MARL Team",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>

  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- Replace with your lab's related works -->
        <a href="https://arxiv.org/abs/2508.03864" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety</h5>
            <p>Pan et al., 2025. Internalizing safety via co-evolution in MARL.</p>
            <span class="work-venue">arXiv 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution</h1>
              <div class="is-size-5 publication-authors">
                Zhenyu Pan<sup>1*</sup>, Yiting Zhang<sup>1*</sup>, Zhuo Liu<sup>2</sup>, Yunlong Tang<sup>3</sup>, Zeliang Zhang<sup>1</sup>, Haozheng Luo<sup>4</sup>, Yuwei Han<sup>1</sup>, Jianshu Zhang<sup>2</sup>, Dennis Wu<sup>4</sup>, Hong-Yu Chen<sup>1</sup>, Haoran Lu<sup>5</sup>, Haoyang Fang<sup>1</sup>, Manling Li<sup>1</sup>, Chenliang Xu<sup>1</sup>, Philip S. Yu<sup>6</sup>, Han Liu<sup>1</sup>
              </div>
              <div class="is-size-5 publication-authors">
                <span class="author-block">¹ Northwestern University  ² University of Illinois at Chicago  ³ University of Rochester  ⁴ Carnegie Mellon University  ⁵ Institution X  ⁶ Institution Y<br>Conference/Preprint: arXiv 2025</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2510.01586.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fas fa-file-pdf"></i></span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/YOUR_REPO/advevo-marl" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="fab fa-github"></i></span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2510.01586" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon"><i class="ai ai-arxiv"></i></span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Project Introduction -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Project Introduction</h2>
            <div class="content has-text-justified">
              <p>
                In today’s large-language-model (LLM) based multi-agent systems, agents exhibit advanced capabilities in planning, tool usage, and role coordination — but they are increasingly vulnerable to jailbreaks, prompt-injection attacks, and adversarial collaboration avenues. Traditional defences either rely on self-verification (each agent filtering unsafe instructions) or on external guard modules policing interactions. These approaches suffer from limited cross-agent insight, added overhead, and single-point failures.
              </p>
              <p>
                We propose <strong>AdvEvo-MARL</strong>, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety by jointly evolving attacker agents (which craft evolving jailbreak-style attacks) and defender/task agents (which learn to perform their tasks while resisting such attacks). A key innovation is the introduction of a <em>public baseline</em> for advantage estimation — within each functional group (attackers or defenders), agents share a group-level mean return to reduce variance and enhance cooperation. Experiments show that our method consistently keeps attack success rates (ASR) below 20% (vs. baselines up to ~38.3%) while maintaining or improving task utility (up to +3.7% improvement). The result: internalized safety and strong utility in LLM-based multi-agent systems, without external guards or added overhead.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

   <!-- Key Results & Conclusions -->
    <section class="section hero">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Key Results & Conclusions</h2>
        <div class="content">
          <h4 class="subtitle is-4">Major Experimental Results</h4>
          <div class="columns is-multiline">
            <div class="column is-half">
              <img src="static/images/result1_placeholder.png" alt="Figure from paper: Attack Success Rate comparison" loading="lazy"/>
              <p><strong>Figure 1:</strong> Attack Success Rate (ASR) across various topologies – our method reduces ASR to <20% vs. baseline ~38%.</p>
            </div>
            <div class="column is-half">
              <img src="static/images/result2_placeholder.png" alt="Figure/Table from paper: Task Accuracy improvement" loading="lazy"/>
              <p><strong>Figure 2:</strong> Task accuracy change – we achieve task accuracy increases up to +3.7% compared to baseline.</p>
            </div>
            <div class="column is-half">
              <img src="static/images/result3_placeholder.png" alt="Table from paper: Ablation study public baseline" loading="lazy"/>
              <p><strong>Table 1:</strong> Ablation study – public baseline mechanism significantly reduces learning variance and improves defender coordination.</p>
            </div>
            <div class="column is-half">
              <img src="static/images/result4_placeholder.png" alt="Figure/Table from paper: Safety-utility trade-off" loading="lazy"/>
              <p><strong>Figure 3:</strong> Safety-utility trade-off – confirms that safety gains do not come at the cost of utility loss.</p>
            </div>
          </div>
    
          <h4 class="subtitle is-4">Conclusions</h4>
          <p>
            This study demonstrates through extensive experiments that embedding safety directly into the learning process of multi-agent systems is a viable and effective strategy. Unlike traditional approaches where improving safety often comes at the expense of task performance, our framework—<strong>AdvEvo-MARL</strong>—achieves both higher safety and maintained or improved utility.
          </p>
          <p>
            First, across diverse attack topologies (chain, tree, complete) and adversarial modes (jailbreak, prompt injection, multi-agent collusion), AdvEvo-MARL consistently drives the Attack Success Rate (ASR) below 20%, outperforming baseline methods which reach ~38%.  
          </p>
          <p>
            Second, on the task-performance front, not only does the framework avoid the common trade-off between defense and utility, but it even yields up to +3.7% improvement in task accuracy in certain scenarios. This indicates that our method enables agents to become safe *and* capable.  
          </p>
          <p>
            Third, from a training and system-design perspective, the introduction of a <em>public-baseline</em> for advantage estimation reduces variance in policy updates and enhances intra-group coordination, while the co-evolutionary interaction between attacker and defender agents fosters robust adaptation to evolving threats.  
          </p>
          <p>
            Finally, from a broader system architecture standpoint, embed­ding safety within the agent framework (rather than relying on external guard modules) provides a lighter, more scalable, and less failure-prone solution. In summary, AdvEvo-MARL offers a unified framework that evolves alongside adversarial threats while preserving strong task performance.  
          </p>
          <p>
            Looking ahead, this work paves the way for safer, more capable multi-agent systems: future directions include extending to long-horizon, multi-task, multi-modal environments, and scaling to larger agent populations and open-ended co-evolution settings.  
          </p>
        </div>
      </div>
    </section>
    <!-- End Key Results & Conclusions -->


    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@article{pan2025advevo,
  title={AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning},
  author={Pan, Zhenyu and Zhang, Yiting and Liu, Zhuo and Tang, Yunlong and Zhang, Zeliang and Luo, Haozheng and Han, Yuwei and Zhang, Jianshu and Wu, Dennis and Chen, Hong-Yu and Lu, Haoran and Fang, Haoyang and Li, Manling and Xu, Chenliang and Yu, Philip S. and Liu, Han},
  journal={arXiv preprint arXiv:2510.01586},
  year={2025}
}</code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.  
                You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br>  
                This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </main>
</body>
</html>
